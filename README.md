# ğŸ§  **Phi-2-FineTuning**: Instruction-Tuned Dialogue Summarization with LoRA & QLoRA

Fine-tune Microsoftâ€™s **Phi-2** model on dialogue summarization tasks using **LoRA** and **QLoRA** techniques. This Colab-friendly project demonstrates efficient model training with quantized weights and parameter-efficient fine-tuningâ€”optimized for memory-constrained environments. ğŸ—¨ï¸ğŸ“š

---

## ğŸ“š **Project Overview**

ğŸ“ **Notebook**: `Phi2_DialogueSummarization_QLoRA.ipynb`
ğŸ§© **Model**: `microsoft/phi-2`
ğŸ“Š **Dataset**: [`neil-code/dialogsum-test`](https://huggingface.co/datasets/neil-code/dialogsum-test)

This project demonstrates how to:

* Load a large language model in **4-bit precision** (QLoRA)
* Apply **Low-Rank Adaptation** (LoRA) to fine-tune selected layers
* Format and preprocess dialogue data for summarization
* Train efficiently using Hugging Faceâ€™s `Trainer` API
* Compare and evaluate fine-tuned results using ROUGE scores
* Save and publish the model to the Hugging Face Model Hub

---

## ğŸš€ **Quickstart**

### ğŸ”§ 1. Clone the Repository

```bash
git clone https://github.com/ArchitJ6/Phi-2-FineTuning.git
cd Phi-2-FineTuning
```

### ğŸ“¦ 2. Install Required Libraries (in Colab or local)

```bash
!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score
```

---

## ğŸ”„ **Training Workflow**

### âœ… Load Dataset

Uses the [DialogSum](https://huggingface.co/datasets/neil-code/dialogsum-test) dataset for multi-turn dialogue summarization.

### âœ… Quantized Model Loading (QLoRA)

Loads `microsoft/phi-2` in **4-bit NF4 quantized mode** using `BitsAndBytesConfig`.

### âœ… Format Data as Instructions

Samples are reformatted into instruction-response style prompts to align with supervised fine-tuning tasks.

### âœ… Tokenization and Filtering

Uses max sequence length and token ID filtering for memory-safe processing.

### âœ… Apply LoRA Configuration

Fine-tuning is applied only to selected layers:

```python
LoraConfig(
  r=32,
  lora_alpha=32,
  lora_dropout=0.05,
  target_modules=["q_proj", "k_proj", "v_proj", "dense"],
  task_type="CAUSAL_LM"
)
```

### âœ… Fine-Tune with Trainer

```python
trainer = Trainer(
  model=peft_model,
  train_dataset=train_dataset,
  eval_dataset=eval_dataset,
  args=training_args,
  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
)
trainer.train()
```

### âœ… Evaluate Results

Generates summaries using both original and fine-tuned models and compares them with ROUGE scores.

### âœ… Save & Upload

Pushes the final model to the Hugging Face Hub for public sharing.

---

## âš™ï¸ **Training Configuration**

| Parameter             | Value              |
| --------------------- | ------------------ |
| Batch Size            | 1                  |
| Gradient Accumulation | 4                  |
| Max Steps             | 1000               |
| Learning Rate         | 2e-4               |
| Quantization          | 4-bit (NF4)        |
| Optimizer             | `paged_adamw_8bit` |
| LoRA Dropout          | 0.05               |

---

## ğŸŒ **Deploy to Hugging Face Hub**

### 1. Login to Hugging Face

```python
from huggingface_hub import login
login(token="your_hf_token")
```

### 2. Push Model

```python
upload_folder(
    folder_path="/content/fine_tuned_phi_model",
    repo_id="ArchitJ6/phi-2-finetune-archit",
    repo_type="model"
)
```

---

## ğŸ§ª **Example Usage**

```python
prompt = "Instruct: Summarize the following conversation.\n[dialogue text]\nOutput:\n"
print(gen(ft_model, prompt))
```

---

## ğŸ™Œ **Acknowledgments**

* ğŸ¤– [Microsoft Phi-2](https://huggingface.co/microsoft/phi-2)
* ğŸ§  [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
* ğŸ”§ [QLoRA](https://arxiv.org/abs/2305.14314)
* ğŸ“š [Hugging Face Transformers](https://huggingface.co/transformers/)
* â˜ï¸ [Google Colab](https://colab.research.google.com/) for training support

---

## ğŸ“ **License**

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more information.